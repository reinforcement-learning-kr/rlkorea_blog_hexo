---
title: Deterministic Policy Gradient Algorithms
date: 2018-06-16 17:21:48
tags: ["í”„ë¡œì íŠ¸", "í”¼ì§€ì—¬í–‰"]
categories: í”„ë¡œì íŠ¸
author: ì´ì›…ì›
subtitle: í”¼ì§€ì—¬í–‰ 2ë²ˆì§¸ ë…¼ë¬¸
---

Authors: David Silver<sup>1)</sup>, Guy Lever<sup>2)</sup>, Nicloas Heess<sup>1)</sup>, Thomas Degris<sup>1)</sup>, Daan Wierstra<sup>1)</sup>, Martin Riedmiller<sup>1)</sup>
Affiliation: 1) Google Deepmind, 2) University College London (UCL)
Proceeding: International Conference on Machine Learning (ICML) 2014

* [Deterministic Policy Gradient Algorithms](#deterministic-policy-gradient-algorithms)
	* [Summary](#summary)
	* [Background](#background)
		* [Performance objective function](#performance-objective-function)
		* [SPG Theorem](#spg-theorem)
		* [Stochastic Actor-Critic Algorithms](#stochastic-actor-critic-algorithms)
		* [Off-policy Actor-Critic](#off-policy-actor-critic)
	* [Gradient of Deterministic Policies](#gradient-of-deterministic-policies)
		* [Regulariy Conditions](#regulariy-conditions)
		* [Deterministic Policy Gradient Theorem](#deterministic-policy-gradient-theorem)
		* [DPG í˜•íƒœì— ëŒ€í•œ informal intuition](#dpg-í˜•íƒœì—-ëŒ€í•œ-informal-intuition)
		* [DPG ëŠ” SPG ì˜ limiting case ì„](#dpg-ëŠ”-spg-ì˜-limiting-case-ì„)
	* [Deterministic Actor-Critic Algorithms](#deterministic-actor-critic-algorithms)
	* [Experiments](#experiments)
		* [Continuous Bandit](#continuous-bandit)
		* [Continuous Reinforcement Learning](#continuous-reinforcement-learning)
		* [Octopus Arm](#octopus-arm)

<!-- /code_chunk_output -->

<br>

---
## 1. Summary
- Deterministic Policy Gradient (DPG) Theorem ì œì•ˆí•¨ [[Theorem 1](#deterministic-policy-gradient-theorem)]
    1) DPGëŠ” ì¡´ì¬í•˜ë©°,
    2) DPGëŠ” Expected gradient of the action-value functionì˜ í˜•íƒœë¥¼ ëˆë‹¤.
- Policy varianceê°€ 0ì— ìˆ˜ë ´í•  ê²½ìš°, DPGëŠ” Stochastic Policy Gradient(SPG)ì™€ ë™ì¼í•´ì§ì„ ë³´ì„ [[Theorem 2](#dpg-ëŠ”-spg-ì˜-limiting-case-ì„)]
    - Theorem 2 ë¡œ ì¸í•´ ê¸°ì¡´ Policy Gradient (PG) ì™€ ê´€ë ¨ëœ ê¸°ë²•ë“¤ì„ DPG ì— ì ìš©í•  ìˆ˜ ìˆê²Œ ë¨
        - ì˜ˆ. Sutton PG, natural gradients, actor-critic, episodic/batch methods
- ì ì ˆí•œ exploration ì„ ìœ„í•´ model-free, off-policy Actor-Critic algorithm ì„ ì œì•ˆí•¨
    - Action-value function approximator ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ policy gradientê°€ biasë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ compatibility conditionsì„ ì œê³µ [[Theorem 3](##deterministic-actor-critic-algorithms)]
- DPG ëŠ” SPG ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŒ
    - íŠ¹íˆ high dimensional action spaces ì„ ê°€ì§€ëŠ” tasksì—ì„œì˜ ì„±ëŠ¥ í–¥ìƒì´ í¼
        - SPGì˜ policy gradientëŠ” stateì™€ action spaces ëª¨ë‘ì— ëŒ€í•´ì„œ, DPGì˜ policy gradientëŠ”  state spacesì— ëŒ€í•´ì„œë§Œ í‰ê· ì„ ì·¨í•¨
        - ê²°ê³¼ì ìœ¼ë¡œ, action spacesì˜ dimensionì´ ì»¤ì§ˆìˆ˜ë¡ data efficiencyê°€ ë†’ì€ DPGì˜ í•™ìŠµì´ ë” ì˜ ì´ë¤„ì§€ê²Œ ë¨
        - ë¬´í•œì • í•™ìŠµì„ ì‹œí‚¤ê²Œ ë˜ë©´, SPGë„ ìµœì ìœ¼ë¡œ ìˆ˜ë ´í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ê¸°ì— ìœ„ ì„±ëŠ¥ ë¹„êµëŠ” ì¼ì • iteration ë‚´ë¡œ í•œì •í•¨
    - ê¸°ì¡´ ê¸°ë²•ë“¤ì— ë¹„í•´ computation ì–‘ì´ ë§ì§€ ì•ŠìŒ
        - Computation ì€ action dimensionality ì™€ policy parameters ìˆ˜ì— ë¹„ë¡€í•¨
<br>

---
## 2. Background
### 2.1 Performance objective function

$$
\begin{align}
J(\pi_{\theta}) &= \int_{S}\rho^{\pi}(s)\int_{A}\pi_{\theta}(s,a)r(s,a)dads = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[r(s,a)]
\end{align}
$$

### 2.2 SPG Theorem
- State distribution $ \rho^{\pi}(s) $ ì€ policy parametersì— ì˜í–¥ì„ ë°›ì§€ë§Œ, policy gradient ë¥¼ ê³„ì‚°í•  ë•ŒëŠ” state distribution ì˜ gradient ë¥¼ ê³ ë ¤í•  í•„ìš”ê°€ ì—†ë‹¤.
- $$\begin{eqnarray}\nabla_{\theta}J(\pi_{\theta}) &=& \int_{S}\rho^{\pi}(s)\int_{A}\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)dads \nonumber \\ &=& E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(a|s)Q^{\pi}(s,a)]
\end{eqnarray}$$

### 2.3 Stochastic Actor-Critic Algorithms
- Actor ì™€ Critic ì´ ë²ˆê°ˆì•„ê°€ë©´ì„œ ë™ì‘í•˜ë©° stochastic policy ë¥¼ ìµœì í™”í•˜ëŠ” ê¸°ë²•
- Actor: $ Q^{\pi}(s,a) $ ë¥¼ ê·¼ì‚¬í•œ $ Q^w(s,a) $ ë¥¼ ì´ìš©í•´ stochastic policy gradient ë¥¼ ascent í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ policy parameter $ \theta $ ë¥¼ ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨ stochastic policy ë¥¼ ë°œì „ì‹œí‚´
    - $ \nabla_{\theta}J(\pi_{\theta}) = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(a|s)Q^{w}(s,a)] $
- Critic: SARSA ë‚˜ Q-learning ê°™ì€ Temporal-difference (TD) learning ì„ ì´ìš©í•´ action-value functionì˜ parameter, $ w $ ë¥¼ ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨ $ Q^w(s,a) $ ê°€ $ Q^{\pi}(s,a) $ ê³¼ ìœ ì‚¬í•´ì§€ë„ë¡ í•¨
- ì‹¤ì œ ê°’ì¸ $ Q^{\pi}(s,a) $ ëŒ€ì‹  ì´ë¥¼ ê·¼ì‚¬í•œ $ Q^w(s,a) $ ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ì¼ë°˜ì ìœ¼ë¡œ bias ê°€ ë°œìƒí•˜ê²Œ ëœë‹¤. í•˜ì§€ë§Œ, compatible condition ì— ë¶€í•©í•˜ëŠ” $ Q^w(s,a) $ ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, bias ê°€ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤.

### 2.4 Off-policy Actor-Critic
- Distinct behavior policy $ \beta(a|s) ( \neq \pi_{\theta}(a|s) ) $ ë¡œë¶€í„° ìƒ˜í”Œë§ëœ trajectories ë¥¼ ì´ìš©í•œ Actor-Critic
- Performance objective function
    - $\begin{eqnarray}
        J_{\beta}(\pi_{\theta}) 
        &=& \int_{S}\rho^{\beta}(s)V^{\pi}(s)ds \nonumber \\\\
        &=& \int_{S}\int_{A}\rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads
        \end{eqnarray} $
- off-policy policy gradient
    - $ \begin{eqnarray}
        \nabla_{\theta}J_{\beta}(\pi_{\theta}) &\approx& \int_{S}\int_{A}\rho^{\beta}(s)\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)dads \nonumber \end{eqnarray} $
      $=E_{s \sim \rho^{\beta}, a \sim \beta}[\frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)}\nabla_{\theta}log\pi_{\theta}(a|s)Q^{\pi}(s,a)]$
    - off-policy policy gradient ì‹ì—ì„œì˜ ë¬¼ê²° í‘œì‹œëŠ” [Degris, 2012b] ë…¼ë¬¸ì— ê·¼ê±°í•¨
        - [Degris, 2012b] "Linear off-policy actor-critic," ICML 2012
        - Exact off-policy policy gradient ì™€ ì´ë¥¼ approximate í•œ policy gradient ëŠ” ì•„ë˜ì™€ ê°™ìŒ. (ë¹¨ê°„ìƒ‰ ìƒìì— ìˆëŠ” í•­ëª©ì„ ì‚­ì œí•¨ìœ¼ë¡œì¨ ê·¼ì‚¬í•¨)
            - <img src="https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1" width=500px>
        - [Degris, 2012b] Theorem 1 ì— ì˜í•´ policy parameter ê°€ approximated policy gradient ( $\nabla_{u}ğ‘„^{\pi,\gamma}(ğ‘ ,ğ‘)$ term ì œê±°)ì— ë”°ë¼ ì—…ë°ì´íŠ¸ë˜ì–´ë„ policy ëŠ” improve ê°€ ë¨ì´ ë³´ì¥ë˜ê¸°ì— exact off-policy policy gradient ëŒ€ì‹  approximated off-policy policy gradient ë¥¼ ì‚¬ìš©í•´ë„ ê´œì°®ìŒ.
            - <img src="https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1" width=500px>
    - off-policy policy gradient ì‹ì—ì„œ $ \frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)} $ ëŠ” importance sampling ratio ì„
        - off-policy actor-critic ì—ì„œëŠ” $ \beta $ ì— ì˜í•´ ìƒ˜í”Œë§ëœ trajectory ë¥¼ ì´ìš©í•´ì„œ stochastic policy $ \pi $ ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— imnportance sampling ì´ í•„ìš”í•¨
<br>

---
## 3. Gradient of Deterministic Policies
### 3.1 Regulariy Conditions
- ì–´ë– í•œ ì´ë¡ ì´ ì„±ë¦½í•˜ê¸° ìœ„í•œ ì „ì œ ì¡°ê±´
- Regularity conditions A.1
    - $ p(s'|s,a), \nabla_{a}p(s'|s,a), \mu_{\theta}(s), \nabla_{\theta}\mu_{\theta}(s), r(s,a), \nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.
- regularity conditions A.2
    - There exists a $ b $ and $ L $ such that $ \sup_{s}p_{1}(s) < b $, $ \sup_{a,s,s'}p(s'|s,a) < b $, $ \sup_{a,s}r(s,a) < b $, $ \sup_{a,s,s'}\|\nabla_{a}p(s'|s,a)\| < L $, and $ \sup_{a,s}\|\nabla_{a}r(s,a)\| < L $.

### 3.2 Deterministic Policy Gradient Theorem
- Deterministic policy
    - $ \mu_{\theta} : S \to A $ with parameter vector $ \theta \in \rm I \! R^n $
- Probability distribution
    - $ p(s \to s', t, \mu) $
- Discounted state distribution
    - $ \rho^{\mu}(s) $
- Performance objective

$$
J(\mu_{\theta}) = E[r^{\gamma}_{1} | \mu] 
$$

$$
= \int_{S}\rho^{\mu}(s)r(s,\mu_{\theta}(s))ds 
= E_{s \sim \rho^{\mu}}[r(s,\mu_{\theta}(s))]
$$

- DPG Theorem
    - MDP ê°€ A.1 ë§Œì¡±í•œë‹¤ë©´, ì•„ë˜ ì‹(9)ì´ ì„±ë¦½í•¨
    $\nabla_{\theta}J(\mu_{\theta}) = \int_{S}\rho^{\mu}(s)\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}ds \nonumber$
    $= E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}]   \nonumber $ (9)
    
	- DPGëŠ” State space ì— ëŒ€í•´ì„œë§Œ í‰ê· ì„ ì·¨í•˜ë©´ ë˜ê¸°ì—, Stateì™€ Action space ëª¨ë‘ì— ëŒ€í•´ í‰ê· ì„ ì·¨í•´ì•¼ í•˜ëŠ” [SPG](#spg-theorem)ì— ë¹„í•´ data efficiencyê°€ ì¢‹ë‹¤. ì¦‰, ë” ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ í•™ìŠµì´ ì˜ ì´ë¤„ì§€ê²Œ ëœë‹¤.

    
### 3.3 DPG í˜•íƒœì— ëŒ€í•œ informal intuition
- Generalized policy iteration
    - ì •ì±… í‰ê°€ì™€ ì •ì±… ë°œì „ì„ í•œ ë²ˆ ì”© ë²ˆê°ˆì•„ ê°€ë©´ì„œ ì‹¤í–‰í•˜ëŠ” ì •ì±… iteration
        - ìœ„ì™€ ê°™ì´ í•´ë„ ì •ì±… í‰ê°€ì—ì„œ ì˜ˆì¸¡í•œ ê°€ì¹˜í•¨ìˆ˜ê°€ ìµœì  ê°€ì¹˜í•¨ìˆ˜ì— ìˆ˜ë ´í•¨
- ì •ì±… í‰ê°€
    - action-value function $ Q^{\pi}(s,a) $ or $ Q^{\mu}(s,a) $ ì„ estimate í•˜ëŠ” ê²ƒ
- ì •ì±… ë°œì „
    - ìœ„ estimated action-value function ì— ë”°ë¼ ì •ì±…ì„ update í•˜ëŠ” ê²ƒ
    - ì£¼ë¡œ action-value function ì— ëŒ€í•œ greedy maximisation ì‚¬ìš©í•¨
        - $ \mu^{k+1}(s) = \arg\max\limits_{a}Q^{\mu^{k}}(s,a) $
        - greedy ì •ì±… ë°œì „ì€ ë§¤ ë‹¨ê³„ë§ˆë‹¤ global maximizationì„ í•´ì•¼í•˜ëŠ”ë°, ì´ëŠ” continuous action spaces ì—ì„œ ê³„ì‚°ëŸ‰ì´ ê¸‰ê²©íˆ ëŠ˜ì–´ë‚˜ê²Œ ë¨.
    - ê·¸ë ‡ê¸°ì— policy gradient ë°©ë²•ì´ ë‚˜ì˜´
        - policy ë¥¼ $ \theta $ ì— ëŒ€í•´ì„œ parameterize í•¨
        - ë§¤ ë‹¨ê³„ë§ˆë‹¤ global maximisation ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ , ë°©ë¬¸í•˜ëŠ” state $ s $ ë§ˆë‹¤ policy parameter ë¥¼ action-value function $ Q $ ì˜ $ \theta $ ì— ëŒ€í•œ gradient $ \nabla_{\theta}Q^{\mu^{k}}(s,\mu_{\theta}(s)) $ ë°©í–¥ìœ¼ë¡œ proportional í•˜ê²Œ update í•¨
        - í•˜ì§€ë§Œ ê° state ëŠ” ë‹¤ë¥¸ ë°©í–¥ì„ ì œì‹œí•  ìˆ˜ ìˆê¸°ì—, state distribution $ \rho^{\mu}(s) $ ì— ëŒ€í•œ ê¸°ëŒ€ê°’ì„ ì·¨í•´ policy parameter ë¥¼ update í•  ìˆ˜ë„ ìˆìŒ
            - $ \theta^{k+1} = \theta^{k} + \alpha \rm I\!E_{s \sim \rho^{\mu^{k}}} [\nabla_{\theta}Q^{\mu^{k}}(s,\mu_{\theta}(s))] $
        - ì´ëŠ” chain-rule ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ ë¶„ë¦¬ë  ìˆ˜ ìˆìŒ
            - $ \theta^{k+1} = \theta^{k} + \alpha \rm I\!E_{s \sim \rho^{\mu^{k}}} [\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu^{k}}(s,a)\vert_{a=\mu_{\theta}(s)}] $ (7)
            - chain rule: $ \frac{\partial Q}{\partial \theta} = \frac{\partial a}{\partial \theta} \frac{\partial Q}{\partial a} $
        - í•˜ì§€ë§Œ state distribution $ \rho^{\mu} $ ì€ ì •ì±…ì— dependent í•¨
            - ì •ì±…ì´ ë°”ê¾¸ê²Œ ë˜ë©´, ë°”ë€ ì •ì±…ì— ë”°ë¼ ë°©ë¬¸í•˜ê²Œ ë˜ëŠ” state ê°€ ë³€í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— state distributionì´ ë³€í•˜ê²Œ ë¨
        - ê·¸ë ‡ê¸°ì— ì •ì±… update ì‹œ state distributionì— ëŒ€í•œ gradient ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë° ì •ì±… ë°œì „ì´ ì´ë¤„ì§„ë‹¤ëŠ” ê²ƒì€ ì§ê´€ì ìœ¼ë¡œ ì™€ë‹¿ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        - deterministic policy gradient theorem ì€ state distributionì— ëŒ€í•œ gradient ê³„ì‚°ì—†ì´ ìœ„ ì‹(7) ëŒ€ë¡œë§Œ update í•´ë„ performance objective ì˜ gradient ë¥¼ ì •í™•í•˜ê²Œ ë”°ë¦„ì„ ì˜ë¯¸í•œë‹¤.


### 3.4 DPG ëŠ” SPG ì˜ limiting case ì„
- stochastic policy parameterization
    - $ \pi_{\mu_{\theta},\sigma} $ by a deterministic policy $ \mu_{\theta} : S \to A $ and a variance parameter $ \sigma $
    - $ \sigma = 0 $ ì´ë©´, $ \pi_{\mu_{\theta},\sigma} \equiv \mu_{\theta} $
- Theorem 2. Policy ì˜ variance ê°€ 0 ì— ìˆ˜ë ´í•˜ë©´, ì¦‰, $ \sigma \to 0 $, stochastic policy gradient ì™€ deterministic policy gradient ëŠ” ë™ì¼í•´ì§
    - ì¡°ê±´ : stochastic policy $ \pi_{\mu_{\theta},\sigma} = \nu_{\sigma}(\mu_{\theta}(s),a) $
        - $ \sigma $ ëŠ” variance
        - $ \nu_{\sigma}(\mu_{\theta}(s),a) $ ëŠ” conditions B.1 ë§Œì¡±
        - MDP ëŠ” conditions A.1 ë° A.2 ë§Œì¡±
    - ê²°ê³¼ :
        - $ \lim\limits_{\sigma\downarrow0}\nabla_{\theta}J(\pi_{\mu_{\theta},\sigma}) = \nabla_{\theta}J(\mu_{\theta})  $
            - ì¢Œë³€ì€ standard stochastic gradient ì´ë©°, ìš°ë³€ì€ deterministic gradient.
    - ì˜ë¯¸ :
        - deterministic policy gradient ëŠ” stochastic policy gradient ì˜ íŠ¹ìˆ˜ case ì„
        - ê¸°ì¡´ ìœ ëª…í•œ policy gradients ê¸°ë²•ë“¤ì— deterministic policy gradients ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŒ
            - ê¸°ì¡´ ê¸°ë²•ë“¤ ì˜ˆ: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)
<br>
 
---
## 4. Deterministic Actor-Critic Algorithms
1. ì‚´ì‚¬ critic ì„ ì´ìš©í•œ on-policy actor-critic
    - ë‹¨ì 
        - deterministic policy ì— ì˜í•´ í–‰ë™í•˜ë©´ exploration ì´ ì˜ ë˜ì§€ ì•Šê¸°ì—, sub-optimal ì— ë¹ ì§€ê¸° ì‰¬ì›€
    - ëª©ì 
        - êµí›ˆ/ì •ë³´ì œê³µ
        - í™˜ê²½ì—ì„œ ì¶©ë¶„í•œ noise ë¥¼ ì œê³µí•˜ì—¬ explorationì„ ì‹œí‚¬ ìˆ˜ ìˆë‹¤ë©´, deterministic policy ë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•˜ì—¬ë„ ì¢‹ì€ í•™ìŠµ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ë„ ìˆìŒ$
            - ì˜ˆ. ë°”ëŒì´ agentì˜ í–‰ë™ì— ì˜í–¥(noise)ì„ ì¤Œ
    - Remind: ì‚´ì‚¬(SARSA) update rule
        - $ Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha(r_{t} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $
    - Algorithm
        - Critic ì€ MSE ë¥¼ $ \bf minimize $ í•˜ëŠ” ë°©í–¥, ì¦‰, action-value function ì„ stochastic gradient $ \bf descent $ ë°©ë²•ìœ¼ë¡œ update í•¨
            - $ MSE = [Q^{\mu}(s,a) - Q^{w}(s,a)]^2 $
                - critic ì€ ì‹¤ì œ $ Q^{\mu}(s,a) $ ëŒ€ì‹  ë¯¸ë¶„ ê°€ëŠ¥í•œ $ Q^{w}(s,a) $ ë¡œ ëŒ€ì²´í•˜ì—¬ action-value function ì„ estimate í•˜ë©°, ì´ ë‘˜ ê°„ Mean Square Error ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œ
            - $ \nabla_{w}MSE \approx -2 * [r + \gamma Q^{w}(s',a') - Q^{w}(s,a)]\nabla_{w}Q^{w}(s,a)  $
                - $ \nabla_{w}MSE = -2 * [Q^{\mu}(s,a) - Q^{w}(s,a)]\nabla_{w}Q^{w}(s,a)  $
                - $ Q^{\mu}(s,a) $ ë¥¼ $ r + \gamma Q^{w}(s',a') $ ë¡œ ëŒ€ì²´
                    - $ Q^{\mu}(s,a) = r + \gamma Q^{\mu}(s',a') $
            - $ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}Q^{w}(s_{t},a_{t}) $
                - $w_{t+1} = w_{t} - \alpha_{w}\nabla_{w}MSE  \nonumber$
                $ \approx w_{t} - \alpha_{w} * (-2 * [r + \gamma Q^{w}(s',a') - Q^{w}(s,a)] \nabla_{w}Q^{w}(s,a)$
                - $ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $
        - Actor ëŠ” ì‹(9)ì— ë”°ë¼ ë³´ìƒì´ $ \bf maximize $ ë˜ëŠ” ë°©í–¥, ì¦‰, deterministic policy ë¥¼ stochastic gradient $ \bf ascent $ ë°©ë²•ìœ¼ë¡œ updateí•¨
            - $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})\nabla_{a}Q^{w}(s_{t},a_{t})\vert_{a=\mu_{\theta}(s)} $
2. Q-learning ì„ ì´ìš©í•œ off-policy actor-critic
    - stochastic behavior policy $ \beta(a|s) $ ì— ì˜í•´ ìƒì„±ëœ trajectories ë¡œë¶€í„° deterministic target policy $ \mu_{\theta}(s) $ ë¥¼ í•™ìŠµí•˜ëŠ” off-policy actor-critic
    - performance objective
        - $ J_{\beta}(\mu_{\theta}) = \int_{S}\rho^{\beta}(s)V^{\mu}(s)ds \nonumber \\\\$
          $= \int_{S}\rho^{\beta}(s)Q^{\mu}(s,\mu_{\theta}(s))ds \nonumber \\\\$
          $= E_{s \sim \rho^{\beta}}[Q^{\mu}(s,\mu_{\theta}(s))]$
    - off-policy deterministic policy gradient
        - $ \nabla_{\theta}J_{\beta}(\mu_{\theta}) = E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}] $
            - ë…¼ë¬¸ì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ë‚˜ì™€ìˆëŠ”ë°, ë¬¼ê²° í‘œì‹œ ë¶€ë¶„ì€ ì˜¤ë¥˜ë¡œ íŒë‹¨ë¨.
            - $ \begin{eqnarray}
                \nabla_{\theta}J_{\beta}(\mu_{\theta}) &\approx& \int_{S}\rho^{\beta}(s)\nabla_{\theta}\mu_{\theta}(a|s)Q^{\mu}(s,a)ds \nonumber \\
                &=& E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}]
                \end{eqnarray} $
            - ê·¼ê±°: Actionì´ deterministic í•˜ê¸°ì— stochastic ê²½ìš°ì™€ëŠ” ë‹¤ë¥´ê²Œ performance objective ì—ì„œ action ì— ëŒ€í•´ í‰ê· ì„ êµ¬í•  í•„ìš”ê°€ ì—†ìŒ. ê·¸ë ‡ê¸°ì—, ê³±ì˜ ë¯¸ë¶„ì´ ìˆì„ í•„ìš”ê°€ ì—†ê³ , [Degris, 2012b] ì—ì„œì²˜ëŸ¼ ê³±ì˜ ë¯¸ë¶„ì„ í†µí•´ ìƒê¸°ëŠ” action-value function ì— ëŒ€í•œ gradient term ë¥¼ ìƒëµí•  í•„ìš”ê°€ ì‚¬ë¼ì§. [ì°¸ê³ ](#off-policy-actor-critic)
    - Remind: íëŸ¬ë‹(Q-learning) update rule
        - $ Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha(r_{t} + \gamma \max\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $
    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)
        - ì‚´ì‚¬ë¥¼ ì´ìš©í•œ on-policy deterministic actor-critic ê³¼ ì•„ë˜ ë¶€ë¶„ì„ ì œì™¸í•˜ê³ ëŠ” ê°™ìŒ
            - target policy ëŠ” $ \beta(a|s) $ ì— ì˜í•´ ìƒì„±ëœ trajectories ë¥¼ í†µí•´ í•™ìŠµí•¨
            - ì—…ë°ì´íŠ¸ ëª©í‘œ ë¶€ë¶„ì— ì‹¤ì œ í–‰ë™ ê°’ $ a_{t+1} $ ì´ ì•„ë‹ˆë¼ ì •ì±…ìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ í–‰ë™ ê°’ $ \mu_{\theta}(s_{t+1}) $ ì‚¬ìš©í•¨
                - $ \mu_{\theta}(s_{t+1}) $ ëŠ” ê°€ì¥ ë†’ì€ Q ê°’ì„ ê°€ì§€ëŠ” í–‰ë™. ì¦‰, Q-learning.
        - $ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $
        - $ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}Q^{w}(s_{t},a_{t}) $
        - $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})\nabla_{a}Q^{w}(s_{t},a_{t})\vert_{a=\mu_{\theta}(s)} $
    - Stochastic off-policy actor-critic ì€ ëŒ€ê°œ actor ì™€ critic ëª¨ë‘ importance samplingì„ í•„ìš”ë¡œ í•˜ì§€ë§Œ, deterministic policy gradient ì—ì„  importance samplingì´ í•„ìš”ì—†ìŒ
        - Actor ëŠ” deterministic ì´ê¸°ì— sampling ìì²´ê°€ í•„ìš”ì—†ìŒ
            - Stochastic policy ì¸ ê²½ìš°, Actor ì—ì„œ importance samplingì´ í•„ìš”í•œ ì´ìœ ëŠ” ìƒíƒœ $ s $ ì—ì„œì˜ ê°€ì¹˜ í•¨ìˆ˜ ê°’ $ V^{\pi}(s) $ ì„ estimate í•˜ê¸° ìœ„í•´ $ \pi $ ê°€ ì•„ë‹ˆë¼ $ \beta $ ì— ë”°ë¼ samplingì„ í•œ í›„, í‰ê· ì„ ë‚´ê¸° ë•Œë¬¸ì„.
            - í•˜ì§€ë§Œ Deterministic policy ì¸ ê²½ìš°, ìƒíƒœ $ s $ ì—ì„œì˜ ê°€ì¹˜ í•¨ìˆ˜ ê°’ $ V^{\pi}(s) = Q^{\pi}(s,\mu_{\theta}) $ ì¦‰, action ì´ ìƒíƒœ s ì— ëŒ€í•´ deterministic ì´ê¸°ì— sampling ì„ í†µí•´ estimate í•  í•„ìš”ê°€ ì—†ê³ , ë”°ë¼ì„œ importance samplingë„ í•„ìš”ì—†ì–´ì§.
            - stochastic vs. deterministic performance objective
                - stochastic : $ J_{\beta}(\mu_{\theta}) = \int_{S}\int_{A}\rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads $
                - deterministic : $ J_{\beta}(\mu_{\theta}) = \int_{S}\rho^{\beta}(s)Q^{\mu}(s,\mu_{\theta}(s))ds $
        - Critic ì´ ì‚¬ìš©í•˜ëŠ” Q-learning ì€ importance samplingì´ í•„ìš”ì—†ëŠ” off policy ì•Œê³ ë¦¬ì¦˜ì„.
            - Q-learning ë„ ì—…ë°ì´íŠ¸ ëª©í‘œë¥¼ íŠ¹ì • ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ì„ í†µí•´ estimate í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Q í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ê¸°ì— ìœ„ actor ì—ì„œì˜ deterministic ê²½ìš°ì™€ ë¹„ìŠ·í•˜ê²Œ ë³¼ ìˆ˜ ìˆìŒ
3. compatible function approximation ë° gradient temporal-difference learning ì„ ì´ìš©í•œ actor-critic
    - ìœ„ ì‚´ì‚¬/Q-learning ê¸°ë°˜ on/off-policy ëŠ” ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œê°€ ì¡´ì¬
        - function approximator ì— ì˜í•œ bias
            - ì¼ë°˜ì ìœ¼ë¡œ $ Q^{\mu}(s,a) $ ë¥¼ $ Q^{w}(s,a) $ ë¡œ ëŒ€ì²´í•˜ì—¬ deterministic policy gradient ë¥¼ êµ¬í•˜ë©´, ê·¸ gradient ëŠ” ascent í•˜ëŠ” ë°©í–¥ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ
        - off-policy learning ì— ì˜í•œ instabilities
    - ê·¸ë˜ì„œ stochastic ì²˜ëŸ¼ $ \nabla_{a}Q^{\mu}(s,a) $ ë¥¼ $ \nabla_{a}Q^{w}(s,a) $ ë¡œ ëŒ€ì²´í•´ë„ deterministic policy gradient ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ compatible function approximator $ Q^{w}(s,a) $ ë¥¼ ì°¾ì•„ì•¼ í•¨
    - Theorem 3. ì•„ë˜ ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´, $ Q^{w}(s,a) $ ëŠ” deterministic policy $ \mu_{\theta}(s) $ ì™€ compatible í•¨. ì¦‰, $ \nabla_{\theta}J_{\beta}(\mu_{\theta}) = E_{s \sim \rho^{\beta}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)}] $
        - $ \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} = \nabla_{\theta}\mu_{\theta}(s)^{\top}w $
        - $ w $ ëŠ” $ MSE(\theta, w) = E[\epsilon(s;\theta,w)^{\top}\epsilon(s;\theta,w)] $ ë¥¼ ìµœì†Œí™”í•¨
            - $ \epsilon(s;\theta,w) = \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} - \nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}  $
    - Theorem 3 ì€ on-policy ë¿ë§Œ ì•„ë‹ˆë¼ off-policy ì—ë„ ì ìš© ê°€ëŠ¥í•¨
    - $ Q^{w}(s,a) = (a-\mu_{\theta}(s))^{\top}\nabla_{\theta}\mu_{\theta}(s)^{\top} w + V^{v}(s) $
        - ì–´ë– í•œ deterministic policy ì— ëŒ€í•´ì„œë„ ìœ„ í˜•íƒœì™€ ê°™ì€ compatible function approximator ê°€ ì¡´ì¬í•¨.
        - ì•ì˜ term ì€ advantage ë¥¼, ë’¤ì˜ term ì€ value ë¡œ ë³¼ ìˆ˜ ìˆìŒ
    - $ Q^{w}(s,a) = \phi(s,a)^{\top} w + v^{\top}\phi(s) $
        - ì •ì˜ : $ \phi(s,a) \overset{\underset{\mathrm{def}}{}}{=} \nabla_{\theta}\mu_{\theta}(s)(a-\mu_{\theta}(s)) $
        - ì¼ë¡€ : $ V^{v}(s) = v^{\top}\phi(s) $
        - Theorem 3 ë§Œì¡± ì—¬ë¶€
            - ì²« ë²ˆì§¸ ì¡°ê±´ ë§Œì¡±.
            - ë‘ ë²ˆì§¸ ì¡°ê±´ì€ ëŒ€ê°• ë§Œì¡±.
                - $ \nabla_{a}Q^{\mu}(s,a) $ ì— ëŒ€í•œ unbiased sample ì„ íšë“í•˜ê¸°ëŠ” ë§¤ìš° ì–´ë µê¸°ì—, ì¼ë°˜ì ì¸ ì •ì±… í‰ê°€ ë°©ë²•ë“¤ë¡œ $ w $ í•™ìŠµ.
                - ì´ ì •ì±… í‰ê°€ ë°©ë²•ë“¤ì„ ì´ìš©í•˜ë©´ $ Q^{w}(s,a) \approx Q^{\mu}(s,a) $ ì¸ reasonable solutionì„ ì°¾ì„ ìˆ˜ ìˆê¸°ì— ëŒ€ê°• $ \nabla_{a}Q^{w}(s,a) \approx \nabla_{a}Q^{\mu}(s,a) $ ì´ ë  ê²ƒ.
        - action-value function ì— ëŒ€í•œ linear function approximator ëŠ” í° ê°’ì„ ê°€ì§€ëŠ” actions ì— ëŒ€í•´ì„  divergeí•  ìˆ˜ ìˆì–´ global í•˜ê²Œ action-values ì˜ˆì¸¡í•˜ê¸°ì—ëŠ” ì¢‹ì§€ ì•Šì§€ë§Œ, local critic ì— ì‚¬ìš©í•  ë•ŒëŠ” ë§¤ìš° ìœ ìš©í•˜ë‹¤.
            - ì¦‰, ì ˆëŒ€ê°’ì´ ì•„ë‹ˆë¼ ì‘ì€ ë³€í™”ëŸ‰ì„ ë‹¤ë£¨ëŠ” gradient method ê²½ìš°ì—” $ A^{w}(s,\mu_{\theta}(s)+\delta) = \delta^{\top}\nabla_{\theta}\mu_{\theta}(s)^{\top}w $ ë¡œ, diverge í•˜ì§€ ì•Šê³ , ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŒ
    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)
        - Critic: ì‹¤ì œ action-value function ì— ëŒ€í•œ linear function approximator ì¸ $ Q^{w}(s,a) = \phi(s,a)^{\top}w $ ë¥¼ estimate
            - $ \phi(s,a) = a^{\top}\nabla_{\theta}\mu_{\theta} $
            - Behavior policy $ \beta(a|s) $ ë¡œë¶€í„° ì–»ì€ samplesë¥¼ ì´ìš©í•˜ì—¬ Q-learning ì´ë‚˜ gradient Q-learning ê³¼ ê°™ì€ off-policy algorithm ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•¨
        - Actor: estimated action-value function ì— ëŒ€í•œ gradient ë¥¼ $ \nabla_{\theta}\mu_{\theta}(s)^{\top}w $ ë¡œ ì¹˜í™˜ í›„, ì •ì±…ì„ ì—…ë°ì´íŠ¸í•¨
        - $ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $
        - $ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\phi(s_{t},a_{t}) $
        - $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $
    - off-policy Q-learningì€ linear function approximationì„ ì´ìš©í•˜ë©´ diverge í•  ìˆ˜ë„ ìˆìŒ
        - $ \mu_{\theta}(s_{t+1}) $ ì´ diverge í•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ìœ¼ë¡œ íŒë‹¨ë¨.
        - ê·¸ë ‡ê¸°ì— simple Q-learning ëŒ€ì‹  ë‹¤ë¥¸ ê¸°ë²•ì´ í•„ìš”í•¨.
    - ê·¸ë ‡ê¸°ì— critic ì— gradient Q-learning ì‚¬ìš©í•œ COPDAC-GQ (Gradient Q-learning critic) algorithm ì œì•ˆ
        - gradient temporal-difference learning ì— ê¸°ë°˜í•œ ê¸°ë²•ë“¤ì€ true gradient descent algorithm ì´ë©°, convergeê°€ ë³´ì¥ë¨. (Sutton, 2009)
            - ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” stochastic gradient descent ë¡œ Mean-squared projected Bellman error (MSPBE) ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ
            - critic ì´ actor ë³´ë‹¤ ë¹ ë¥¸ time-scale ë¡œ update ë˜ë„ë¡ step size ë“¤ì„ ì˜ ì¡°ì ˆí•˜ë©´, critic ì€ MSPBE ë¥¼ ìµœì†Œí™”í•˜ëŠ” parameters ë¡œ converge í•˜ê²Œ ë¨
            - critic ì— gradient temporal-difference learning ì˜ ì¼ì¢…ì¸ gradient Q-learning ì‚¬ìš©í•œ ë…¼ë¬¸ (Maei, 2010)
    - COPDAC-GQ algorithm
        - $ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $
        - $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $
        - $ w_{t+1} = w_{t} + \alpha_{w}\delta_{t}\phi(s_{t},a_{t}) - \alpha_{w}\gamma\phi(s_{t+1}, \mu_{\theta}(s_{t+1}))(\phi(s_{t},a_{t})^{\top} u_{t}) $
        - $ v_{t+1} = v_{t} + \alpha_{v}\delta_{t}\phi(s_{t}) - \alpha_{v}\gamma\phi(s_{t+1})(\phi(s_{t},a_{t})^{\top} u_{t}) $
        - $ u_{t+1} = u_{t} + \alpha_{u}(\delta_{t}-\phi(s_{t}, a_{t})^{\top} u_{t})\phi(s_{t}, a_{t}) $
    - stochastic actor-critic ê³¼ ê°™ì´ ë§¤ time-step ë§ˆë‹¤ update ì‹œ í•„ìš”í•œ ê³„ì‚°ì˜ ë³µì¡ë„ëŠ” $ O(mn) $
        - m ì€ action dimensions, n ì€ number of policy parameters
    - Natural policy gradient ë¥¼ ì´ìš©í•´ deterministic policies ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ
        - $ M(\theta)^{-1}\nabla_{\theta}J(\mu_{\theta}) $ ëŠ” any metric $ M(\theta) $ ì— ëŒ€í•œ our performance objective (ì‹(14)) ì˜ steepest ascent direction ì„ (Toussaint, 2012)
        - Natural gradient ëŠ” Fisher information metric $ M_{\pi}(\theta) $ ì— ëŒ€í•œ steepest ascent direction ì„
            -  $ M_{\pi}(\theta) = E_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}[\nabla_{\theta}\log\pi_{\theta}(a|s)\nabla_{\theta}\log\pi_{\theta}(a|s)^{\top}] $
            - Fisher information metric ì€ policy reparameterization ì— ëŒ€í•´ ë¶ˆë³€ì„ (Bagnell, 2003)
        - deterministic policies ì— ëŒ€í•´ metric ìœ¼ë¡œ $ M_{\mu}(\theta) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}] $ ì„ ì‚¬ìš©.
        	- ì´ëŠ” variance ê°€ 0 ì¸ policy ì— ëŒ€í•œ Fisher information metric ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ
        	- $ \frac{\nabla_{\theta}\pi_{\theta}(a\vert s)}{\pi_{\theta}(a\vert s)}$ ì—ì„œ policy variance ê°€ 0 ì´ë©´, íŠ¹ì • s ì— ëŒ€í•œ $ \pi_{\theta}(a|s)$ ë§Œ 1 ì´ ë˜ê³ , ë‚˜ë¨¸ì§€ëŠ” 0 ì„
        - deterministic policy gradient theorem ê³¼ compatible function approximation ì„ ê²°í•©í•˜ë©´ $ \nabla_{\theta}J(\mu_{\theta}) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}w] $ ì´ ë¨
            - $ \nabla_{\theta}J(\mu_{\theta}) = E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)}] $
            - $ \nabla_{a}Q^{\mu}(s,a)\vert_{a=\mu_{\theta}(s)} \approx \nabla_{a}Q^{w}(s,a)\vert_{a=\mu_{\theta}(s)} = \nabla_{\theta}\mu_{\theta}(s)^{\top}w $
        - ê·¸ë ‡ê¸°ì— steepest ascent direction ì€ $ M_{\mu}(\theta)^{-1}\nabla_{\theta}J_{\beta}(\mu_{\theta}) = w $ ì´ ë¨
            - $ E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}]^{-1}E_{s \sim \rho^{\mu}}[\nabla_{\theta}\mu_{\theta}(s)\nabla_{\theta}\mu_{\theta}(s)^{\top}w] = w $
        - ì´ ì•Œê³ ë¦¬ì¦˜ì€ COPDAC-Q í˜¹ì€ COPDAC-GQ ì—ì„œ $ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta}\mu_{\theta}(s_{t})(\nabla_{\theta}\mu_{\theta}(s_{t})^{\top} w_{t}) $ ì‹ì„ $ \theta_{t+1} = \theta_{t} + \alpha_{\theta}w_{t} $ ë¡œ ë°”ê¿”ì£¼ê¸°ë§Œ í•˜ë©´ ë¨

## Experiments
### Continuous Bandit
- Stochastic Actor-Critic (SAC)ê³¼ COPDAC ê°„ ì„±ëŠ¥ ë¹„êµ ìˆ˜í–‰
    - Action dimensionì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ ì°¨ì´ê°€ ì‹¬í•¨
    - ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ í†µí•´ DPGì˜ data efficiencyê°€ SPGì— ë¹„í•´ ì¢‹ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, ë°˜ë©´, time-stepì´ ì¦ê°€í• ìˆ˜ë¡ SACì™€ COPDAC ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ í†µí•´ ì„±ëŠ¥ ì°¨ì´ê°€ ì‹¬í•˜ë‹¤ëŠ” ê²ƒì€ ì¼ì • time step ë‚´ì—ì„œë§Œ í•´ë‹¹í•˜ëŠ” ê²ƒì´ë¼ê³  ìœ ì¶”í•´ë³¼ ìˆ˜ ìˆìŒ
    - <img src="https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1">

### Continuous Reinforcement Learning
- COPDAC-Qê°€ SACì™€ off-policy stochastic actor-critic(OffPAC-TD) ê°„ ì„±ëŠ¥ ë¹„êµ ìˆ˜í–‰
    - COPDAC-Qì˜ ì„±ëŠ¥ì´ ì•½ê°„ ë” ì¢‹ìŒ
    - COPDAC-Qì˜ í•™ìŠµì´ ë” ë¹¨ë¦¬ ì´ë¤„ì§
    - <img src="https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1">
### Octopus Arm
- ëª©í‘œ: 6 segments octopus arm (20 action dimensions & 50 state dimensions)ì„ controlí•˜ì—¬ targetì„ ë§ì¶”ëŠ” ê²ƒ
    - COPDAC-Q ì‚¬ìš© ì‹œ, action space dimensionì´ í° octopus armì„ ì˜ controlí•˜ì—¬ targetì„ ë§ì¶¤
    - <img src="https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1" width=600px>
    - ê¸°ì¡´ ê¸°ë²•ë“¤ì€ action spaces í˜¹ì€ action ê³¼ state spaces ë‘˜ ë‹¤ ì‘ì€ ê²½ìš°ë“¤ì— ëŒ€í•´ì„œë§Œ ì‹¤í—˜í–ˆë‹¤ê³  í•˜ë©°, ë¹„êµí•˜ê³  ìˆì§€ ì•ŠìŒ.
        - ê¸°ì¡´ ê¸°ë²•ë“¤ì´ 6 segments octopus armì—ì„œ ë™ì‘ì„ ì˜ ì•ˆ í–ˆì„ ê²ƒ ê°™ê¸´í•œë°, ê·¸ë˜ë„ ì‹¤í—˜í•´ì„œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€...ì™œ ì•ˆ í–ˆì„ê¹Œ?
    - 8 segment arm ë™ì˜ìƒì´ ì €ì í™ˆí˜ì´ì§€ì— ìˆë‹¤ê³  í•˜ëŠ”ë°, ì•ˆ ë³´ì„.
- [ì°¸ê³ ] Octopus Arm ì´ë€?
    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)
    - <img src="https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1">
